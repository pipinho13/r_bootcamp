---
title: "Machine Learning With R"
output: html_document
---


```{r, message=FALSE, warning=FALSE }
library(ggplot2)
library(ggthemes)
library(dplyr)
library(corrgram)
library(corrplot)
library(caTools)
library(Amelia)
library(ISLR)
library(class)
library(rpart)
library(rpart.plot)
library(randomForest)
library(e1071)
library(cluster)
library(tm)
library(twitteR)
library(RColorBrewer)
library(wordcloud)
library(MASS)
library(neuralnet)
```

#Linear Regression

```{r}
df<-read.csv("student-mat.csv", sep=";")
head(df)
summary(df)

#Check for NA
any(is.na(df))

#Structure of Data
str(df)

#Numeric only
num.cols<-sapply(df, is.numeric)

#filter the data and calculate the correlation matrix
corr.data<-cor(df[,num.cols])
corrplot(corr.data, method='color')

#Corrgram can deal with factros too
corrgram(df)

corrgram(df, order=TRUE)

corrgram(df, order=TRUE, upper.pane=panel.pie)

#A ggplot
ggplot(df, aes(x=G3))+geom_histogram(bins=20, alpha=0.5, fill="green")
```

###split the Data into Train and Test set

```{r}
set.seed(101)
#Split up Sample
sample<-sample.split(df$G3,SplitRatio = 0.7)
#70% of the data
train<-subset(df, sample==TRUE)

#30% of the data
test<-subset(df, sample==FALSE)

#Train & build the model
model<-lm(G3~., data=train)

summary(model)

#Predictions
G3.predictions<-predict(model,test)

results<-cbind(G3.predictions, test$G3)
colnames(results)<-c('predicted', 'actual')
results<-as.data.frame(results)

#Take care of negative values
to_zero<- function(x){
  if (x<0) {
    return(0)
  } else {return(x)}
  }
    
results$predicted<-sapply(results$predicted, to_zero)

mse<-mean((results$predicted-results$actual)^2)

rmse<-sqrt(mse)

mse
rmse
```


####Bike Project from Kaggle

```{r}

bike<-read.csv('bikeshare.csv')
head(bike)

##EDA


ggplot(bike, aes(temp, count))+geom_point(alpha=0.3, aes(color=temp))+ggtitle("Temperature vs Count")


bike$datetime<-as.POSIXct(bike$datetime)

ggplot(bike, aes(datetime, count))+geom_point(aes(color=temp), alpha=0.5)+ggtitle("Date vs Count and Temperature") ###+scale_color_continuous(low='grey', high='black')

ggplot(bike, aes(factor(season), count))+geom_boxplot((aes(color=factor(season))))+theme_bw()


bike$years<-format(bike$datetime, "%Y")

ggplot(bike, aes(factor(season), count))+geom_boxplot((aes(color=factor(season))))+theme_bw()+facet_grid(.~years)+ggtitle("By Season and Year")


bike$hour<-sapply(bike$datetime, function(x) {format(x,"%H")})


#Scatterplot

ggplot(filter(bike, workingday==1), aes(hour, count))+geom_point()+ggtitle("By Hour")

ggplot(filter(bike, workingday==1), aes(hour, count))+geom_point(aes(color=temp))+ggtitle("By Hour and Temperature")+scale_color_gradientn(colours=c('dark blue' , 'blue', 'light blue', 'yellow', 'orange', 'red'))


ggplot(filter(bike, workingday==1), aes(hour, count))+geom_point(position=position_jitter(w=1, h=0), aes(color=temp))+ggtitle("By Hour and Temperature")+scale_color_gradientn(colours=c('dark blue' , 'blue', 'light blue', 'yellow', 'orange', 'red'))


## build model

bike$hour<-as.numeric(bike$hour)
model<-lm(count~. -casual-registered-datetime-atemp, bike)

summary(model)
```

#Logistic Regression

```{r, warning=FALSE, message=FALSE}

df.train<-read.csv('titanic_train.csv')
print(head(df.train))
print(str(df.train))

missmap(df.train, main='Missing Map', col=c('yellow', 'black'), legend = FALSE)

ggplot(df.train, aes(Survived))+geom_bar()
ggplot(df.train, aes(Pclass))+geom_bar(aes(fill=factor(Pclass)))
ggplot(df.train, aes(Sex))+geom_bar(aes(fill=Sex))
ggplot(df.train, aes(Age))+geom_histogram(alpha=0.5, fill='blue', bins=20)
ggplot(df.train, aes(SibSp))+geom_bar()
ggplot(df.train, aes(Fare))+geom_histogram(fill='green', color='black', alpha=0.5)
ggplot(df.train, aes(Pclass, Age))+geom_boxplot(aes(group=Pclass, fill=factor(Pclass)), alpha=0.2)+scale_y_continuous(breaks=seq(0, 80, by=2))

impute_age<-function(age, class){
  out<-age
  for (i in 1:length(age)) {
    if(is.na(age[i])) {
      if (class[i]==1) {
        out[i]<-37
      } else if (class[i]==2) {
        out[i]<-29
      } else {
        out[i]<-24
      }
    } else{
      out[i]<-age[i]
    }
  }
  return(out)
}

fixed.ages<-impute_age(df.train$Age, df.train$Pclass)
df.train$Age<-fixed.ages
#missmap(df.train, main='Missing Map', col=c('yellow', 'black'), legend = FALSE)

df.train<-dplyr::select(df.train, -PassengerId, -Name, -Ticket, -Cabin)

df.train$Survived<-factor(df.train$Survived)
df.train$Pclass<-factor(df.train$Pclass)
df.train$Parch<-factor(df.train$Parch)
df.train$SibSp<-factor(df.train$SibSp)

log.model<-glm(Survived~., family=binomial(link = 'logit'), data=df.train)
summary(log.model)
fitted.prob<-predict(log.model, data=df.train, type='response')
fitted.results<-ifelse(fitted.prob>0.5,1,0)
misClassErr0r<-mean(fitted.results!=df.train$Survived)
misClassErr0r

#confusion matrix
table(df.train$Survived, fitted.prob>0.5 )

##Adult Example
adult<-read.csv('adult_sal.csv')
head(adult)

##Data Cleaning/ feature engineering
#Combine employer type
unemp<-function(job){
  job<-as.character(job)
  if(job=='Never-worked'|job=='Without-pay') {
    return('Unemployed')
  } else {
    return(job)
  }
}

####
####
adult$type_employer<-sapply(adult$type_employer, unemp)
table(adult$type_employer)

group_emp<-function(job){
  job<-as.character(job)
  if(job=='Local-gov'|job=='State-gov') {
    return('SL-gov')
  } else if(job=='Self-emp-inc'|job=='Self-emp-not-inc') {
    return('self-emp')
  } else {
    return(job)
  }
}

###
adult$type_employer<-sapply(adult$type_employer,group_emp)

```

#KNN

```{r}

##we use the class library
str(Caravan)
any(is.na(Caravan))
purchase<-Caravan[,86]

standardized.Caravan<-scale(Caravan[,-86])

#test
test.index<-1:1000
test.data<-standardized.Caravan[test.index,]
test.purchase<-purchase[test.index]

#train
train.data<-standardized.Caravan[-test.index,]
train.purchase<-purchase[-test.index]


##KNN Model

predicted.purchase<-knn(train.data, test.data, train.purchase, k=1 )

misClassErr0r<-mean(test.purchase!=predicted.purchase)
misClassErr0r


###Choosing a K-VALUE
predicted.purchase<-knn(train.data, test.data, train.purchase, k=3 )

misClassErr0r<-mean(test.purchase!=predicted.purchase)
misClassErr0r

predicted.purchase<-knn(train.data, test.data, train.purchase, k=5 )

misClassErr0r<-mean(test.purchase!=predicted.purchase)
misClassErr0r

predicted.purchase<-NULL
error.rate<-NULL

for (i in 1:20) {
  predicted.purchase<-knn(train.data, test.data, train.purchase, k=i )
  error.rate[i]<-mean(test.purchase!=predicted.purchase)
}

error.rate


##Visualize k elbow method
k.values<-1:20
error.df<-data.frame(error.rate, k.values)
ggplot(error.df, aes(k.values, error.rate))+geom_point()+geom_line(lty='dotted', color='red')


##KNN on iris dataset
stand_features<-scale(iris[, 1:4])
final_data<-cbind(stand_features, iris[5])

### Train Test Split
set.seed((101))

sample<-sample.split(final_data$Species, SplitRati=0.7)
train<-subset(final_data, sample==T)
test<-subset(final_data, sample==F)

predicted_species<-knn(train[1:4], test[1:4], train$Species, k=1)
predicted_species
mean(test$Species!=predicted_species)

predicted.species<-NULL
error.rate<-NULL

for (i in 1:10) {
  predicted.species<-knn(train[1:4], test[1:4], train$Species, k=i )
  error.rate[i]<-mean(test$Species!=predicted.species)
}

error.rate

k.values<-1:10
error.df<-data.frame(error.rate, k.values)
ggplot(error.df, aes(k.values, error.rate))+geom_point()+geom_line(lty='dotted', color='red')
```

##Decision Tree and Random Forest

```{r}
##library(rpart); library(rpart.plot)
str(kyphosis)
head(kyphosis)
tree<-rpart(Kyphosis~., method='class', data=kyphosis)

#printcp(tree)
#plotcp(tree)
#rsq.rpart(tree)
#print(tree)
#summary(tree)
#plot(tree)
#text(tree)
#post(tree)

printcp(tree)
plot(tree, uniform=T, main='Kyphosis Tree')
text(tree, use.n=T, all=T)

prp(tree)


###Random Forest
rf.model<-randomForest(Kyphosis~., data=kyphosis)
print(rf.model)

head(rf.model$predicted)
rf.model$ntree


##Project in College data
df<-College
ggplot(df, aes(Room.Board, Grad.Rate))+geom_point(aes(color=Private), size=2, alpha=0.4)
ggplot(df, aes(F.Undergrad))+geom_histogram(aes(fill=Private), color='black', bins=50)+theme_bw()
ggplot(df, aes(Grad.Rate))+geom_histogram(aes(fill=Private), color='black', bins=50)+theme_bw()
df['Cazenovia College', 'Grad.Rate']<-100
sample<-sample.split(df$Private, SplitRatio = 0.7)
train<-subset(df, sample==T)
test<-subset(df, sample==F)

tree<-rpart(Private~.,  method='class', data=train)
tree_preds<-predict(tree, test)
tree_preds_ii<-predict(tree, test, type='class')
head(tree_preds)
head(tree_preds_ii)
prp(tree)

##Random Forest
rf.model<-randomForest(Private~., data=train, importance=TRUE)
rf.model$confusion
rf.model$importance
rf_preds<-predict(rf.model, test)
table(rf_preds, test$Private)
```

##SVM

```{r}
model<-svm(Species~., data=iris)
summary(model)
pred_values<-predict(model, iris[1:4])
table(pred_values, iris$Species)

###Tune Results
tune_results<-tune(svm, train.x = iris[1:4], train.y=iris[,5], kernel='radial', ranges=list(cost=c(0.1,1,10), gamma=c(0.5,1,2)))
summary(tune_results)

tuned_svm<-svm(Species~., data=iris, kernel='radial', cost=1.5, gamma=0.1)
summary(tuned_svm)
pred_values<-predict(tuned_svm, iris[1:4])
table(pred_values, iris$Species)


### SVM Project
loans<-read.csv('loan_data.csv')
str(loans)

##Convert to factors

loans$credit.policy<-factor(loans$credit.policy)
loans$inq.last.6mths<-factor(loans$inq.last.6mths)
loans$delinq.2yrs<-factor(loans$delinq.2yrs)
loans$pub.rec<-factor(loans$pub.rec)
loans$not.fully.paid<-factor(loans$not.fully.paid)

pl<-ggplot(loans, aes(fico))+geom_histogram(aes(fill=not.fully.paid), color='black', bins = 40, alpha=0.5)+theme_bw()
pl
pl+scale_fill_manual(values=c('green', 'red'))

ggplot(loans, aes(x=factor(purpose)))+geom_bar(aes(fill=not.fully.paid), position='dodge')

ggplot(loans, aes(int.rate, fico))+geom_point(aes(color=not.fully.paid), alpha=0.4)

###TRAIN TEST SPLIT

sample<-sample.split(loans$not.fully.paid, 0.7)
train<-subset(loans, sample==T)
test<-subset(loans, sample==F)

model<-svm(not.fully.paid~., data=train)

summary(model)

predicted_values<-predict(model, test[1:13])
table(predicted_values, test$not.fully.paid)

tuned.results<-tune(svm, train.x=not.fully.paid~., data=train, kernel='radial', ranges=list(cost=c(100,200), gamma=c(0.1)))
summary(tuned.results)

tuned_model<-svm(not.fully.paid~., data=train, cost=100, gamma=0.1)
tuned.predictions<-predict(tuned_model, test[1:13])
table(tuned.predictions, test$not.fully.paid)
```

#K Means Clustering

```{r}

ggplot(iris, aes(Petal.Length, Petal.Width, color=Species))+geom_point(size=4)

iriscluster<-kmeans(iris[,1:4], 3, nstart = 20)
print(iriscluster)

table(iriscluster$cluster, iris$Species)

clusplot(iris, iriscluster$cluster, color=T, shade=T, labels = 0, lines=0)

##Project in K means
df1<-read.csv('winequality-red.csv', sep=';')
df2<-read.csv('winequality-white.csv', sep=';')
head(df1)
df1$label<-c('red')
df2$label<-c('white')
wine<-rbind(df1,df2)

ggplot(wine, aes(residual.sugar))+geom_histogram(aes(fill=label), color='black', bins=50)+scale_fill_manual(values=c('dark red', 'white'))
ggplot(wine, aes(citric.acid))+geom_histogram(aes(fill=label), color='black', bins=50)+scale_fill_manual(values=c('dark red', 'white'))
ggplot(wine, aes(alcohol))+geom_histogram(aes(fill=label), color='black', bins=50)+scale_fill_manual(values=c('dark red', 'white'))
ggplot(wine, aes(citric.acid, residual.sugar))+geom_point(aes(color=label), alpha=0.2)+scale_color_manual(values=c('dark red', 'white'))+theme_dark()
ggplot(wine, aes(volatile.acidity, residual.sugar))+geom_point(aes(color=label), alpha=0.2)+scale_color_manual(values=c('dark red', 'white'))+theme_dark()

clusdata<-wine[,1:12]
winecluster<-kmeans(clusdata, 2)
winecluster$centers

table(wine$labe, winecluster$cluster)
```

#NLP

```{r,  warning=FALSE }

#Connect to twitter
ckey=c('WJz1DjXp8ZYwcBMy4cbiTS9mX')
skey=c('27sGwCB9qtEgnSaLv4Z5hZKjuX9PyUkr774CMVx016cmHOMrlM')
token=c('313313903-nZbOUAuHg4B8HNhuQBPPnXJ1oxHisezKT8g9OtvS')
sectoken=c('oVMp0Xn1yIr1nirqHKD18omFAHELIe9ocv3Jw9WWwfGCp')
setup_twitter_oauth(ckey, skey, token, sectoken)

#Search for soccer
soccer_tweeks<-searchTwitter('soccer', n=1000, lang='en')
soccer_text<-sapply(soccer_tweeks, function(x) x$getText())

#Clean Data
soccer_text<-iconv(soccer_text, 'UTF-8', 'ASCII')

soccer_corpus<-Corpus(VectorSource(soccer_text))

#Document Term Matrix
term_doc_metrix<-TermDocumentMatrix(soccer_corpus,
                                      control = list(removePunctuation = TRUE,
                                                     stopwords = c("soccer","http", stopwords("english")),
                                                     removeNumbers = TRUE,tolower = TRUE))

#AS MATRIX

term.doc.matrix <- as.matrix(term_doc_metrix)
#Get words counts
word.freqs <- sort(rowSums(term.doc.matrix), decreasing=TRUE) 
dm <- data.frame(word=names(word.freqs), freq=word.freqs)

wordcloud(dm$word, dm$freq, random.order=FALSE, colors=brewer.pal(8, "Dark2"))




#Search for python
soccer_tweeks<-searchTwitter('python', n=1000, lang='en')
soccer_text<-sapply(soccer_tweeks, function(x) x$getText())

#Clean Data
soccer_text<-iconv(soccer_text, 'UTF-8', 'ASCII')

soccer_corpus<-Corpus(VectorSource(soccer_text))

#Document Term Matrix
term_doc_metrix<-TermDocumentMatrix(soccer_corpus,
                                      control = list(removePunctuation = TRUE,
                                                     stopwords = c("python","http", stopwords("english")),
                                                     removeNumbers = TRUE,tolower = TRUE))

#AS MATRIX

term.doc.matrix <- as.matrix(term_doc_metrix)
#Get words counts
word.freqs <- sort(rowSums(term.doc.matrix), decreasing=TRUE) 
dm <- data.frame(word=names(word.freqs), freq=word.freqs)

wordcloud(dm$word, dm$freq, random.order=FALSE, colors=brewer.pal(8, "Dark2"))
```

#Neural Networks

```{r}
head(Boston)
str(Boston)
data<-Boston

#Normalize our data
maxs<-apply(data, 2, max)
mins<-apply(data, 2, min)

scaled <- as.data.frame(scale(data, center = mins, scale = maxs - mins))
head(scaled)

##Train and Test Sets

split = sample.split(scaled$medv, SplitRatio = 0.70)

train = subset(scaled, split == TRUE)
test = subset(scaled, split == FALSE)

#For some odd reasons, the neuralnet() function won't accept a formula in the form: y~. that we are used to using. Instead you have to call all the columns added together. Here is some #convience code to help quickly create that formula:

# Get column names
n <- names(train)

# Paste together
f <- as.formula(paste("medv ~", paste(n[!n %in% "medv"], collapse = " + ")))

##Becasue it is continuous and not classification we set linear.output=TRUE
nn <- neuralnet(f,data=train,hidden=c(5,3),linear.output=TRUE)
plot(nn)

###Predictions

# Compute Predictions off Test Set
predicted.nn.values <- compute(nn,test[1:13])

# Convert back to non-scaled predictions
true.predictions <- predicted.nn.values$net.result*(max(data$medv)-min(data$medv))+min(data$medv)

# Convert the test data
test.r <- (test$medv)*(max(data$medv)-min(data$medv))+min(data$medv)



# Check the Mean Squared Error
MSE.nn <- sum((test.r - true.predictions)^2)/nrow(test)
MSE.nn


#Visualize Error
error.df <- data.frame(test.r,true.predictions)

ggplot(error.df,aes(x=test.r,y=true.predictions)) + geom_point() + stat_smooth()


########
###Neural Netwrok Project
df <- read.csv('bank_note_data.csv')
head(df)

#TRAIN AND TEST SPLIT
set.seed(101)
split = sample.split(df$Class, SplitRatio = 0.70)

train = subset(df, split == TRUE)
test = subset(df, split == FALSE)

nn <- neuralnet(Class ~ Image.Var + Image.Skew + Image.Curt + Entropy,data=train,hidden=10,linear.output=FALSE)
predicted.nn.values <- compute(nn,test[,1:4])
head(predicted.nn.values$net.result)
#Apply the round function to the predicted values so you only 0s and 1s as your predicted classes.
predictions <- sapply(predicted.nn.values$net.result,round)

head(predictions)

table(predictions,test$Class)

##You should have noticed that you did very well! Almost suspiciously well! Let's check our results against a randomForest model!
##Comparing Models

df$Class <- factor(df$Class)

set.seed(101)
split = sample.split(df$Class, SplitRatio = 0.70)

train = subset(df, split == TRUE)
test = subset(df, split == FALSE)

model <- randomForest(Class ~ Image.Var + Image.Skew + Image.Curt + Entropy,data=train)
rf.pred <- predict(model,test)
table(rf.pred,test$Class)

#Use predict() to get the predicted values from your rf model.

rf.pred <- predict(model,test)
```


